# 大模型漏洞

## 模型层安全

### 数据投毒

1. 开源第三方数据集投毒，企业没有进行良好的数据清洗

2. 大模型发布后重新收集用户的使用数据进行重新训练，也可以完成投毒操作

### 后门植入

1. 第三方开源的大模型被后门植入，可能导致大模型使用方的隐私泄露

比如后门程序将返回的内容进行关键拼接，将信息发送到攻击者的服务器上

### 对抗攻击

又称越狱

用户输入一段专为大模型构造的提示词，绕过LLM的安全或者道德模块，使其能够看到非法内容

利用GPT的本能规避和记忆上下文等进行注入



### 数据泄露

1. 训练的数据集未脱敏

2. LLM本身的关键信息未脱敏

## 框架层安全

二进制安全，一般大模型都是Python为主的各类接口来访问底层的C++实现，攻击者可能通过开发者追求效率而舍弃安全的情况进行内存注入。



处理不可信数据：原始训练数据和序列化存储模型

利用模型处理文件数据的过程进行攻击

反序列化模型中执行恶意代码

训练集群中的主机暴露接口

## 应用层安全

### 传统web安全

未授权、身份验证等问题，攻击者可以访问其他人的敏感数据



或者是通信数据直接明文存储在公网上





大模型本身的漏洞处理一些经过设计的文件会触发漏洞进而进行攻击





沙箱逃逸和任意代码执行



## OWASP-2025发布的大模型十大安全威胁

#### 无限资源消耗（LLM10 Unbounded Consumption）

攻击者发送大量无用请求，恶意占用大模型服务资源，或者发送大量超过模型上下文窗口限制的请求，导致服务降级或不可用（输入泛滥与溢出）。这类风险一般发生在服务接口层。

甚至有些攻击者发送精心设计的复杂prompt，这些prompt会让后端模型计算时占用大量系统资源，导致服务超时或不可用（资源密集型查询）。这类风险一般发生在模型层，属于模型原生风险漏洞。

攻击者通过频繁操作调用基于云提供AI的服务（按量收费），导致服务提供者账单爆炸（钱包拒绝服务）。这类风险一般发生在模型部署层。

攻击者通过构造海量请求prompt套取模型回复形成答案对，从而倒推模型参数（模型窃取/模型复制）。这类风险一般发生在接口服务层。



#### 信息误导（LLM09 Misinformation）

大模型的幻觉问题，包括生成事实性错误（传递信息有误）、无中生有（LLM虚构假的法律安检）、专业能力错误（医疗、金融领域）。

大模型有时候还建议使用不安全的代码或者第三方库，容易引发漏洞。这类风险往往发生在用户和模型系统的交界面。



#### 提示词注入（LLM01 Prompt Injection）

攻击者通过在prompt中植入复杂指令，诱导模型服务生成违反安全规范的内容，或启用未经授权的访问。

提示词注入的类型有很多，常见的有代码注入、对抗性后缀、多语言混淆攻击等等。这类风险一般都发生在大模型服务的接口层。



#### 过度代理（LLM06 Excessive Agency）

大模型服务功能过多、或者权限过高，例如大模型服务除了有读取文档知识库的权限，还有删除、修改的权限，这可能会导致文档内容失控的风险。

另外，大模型或者Agent自主性过强，无需人工干预即可自动对敏感内容文档进行增删改查操作，有很大的潜在风险。

这类风险往往存在于Agent代理层，或者大模型服务调用的各种插件中。



#### 向量与嵌入漏洞（LLM08 Vector & Embedding Weakness）

知识库中含有敏感信息，或者鉴权机制设计不足导致敏感信息泄露（数据泄漏）。

知识库中被有意或者无意投毒，导致模型输出泄露敏感信息或者输出被恶意操控（知识库数据投毒）。

多租户共用相同数据库时，攻击者可以通过上下文获取其他用户聊天信息（跨上下文信息泄露）。



#### 敏感信息泄露（LLM02 Sensitive Info Disclosure）

敏感信息包括个人隐私信息（PII）、商业隐私信息和模型自身的隐私信息。

这类风险可能发生在用户和模型交互的过程中，也可能发生在训练阶段，算法工程师将未经过滤的敏感信息放入训练数据（预训练/微调数据）。



#### 系统提示泄漏（LLM07 System Prompt Leakage）

系统提示信息包括敏感功能（数据库类型）、内部规则（业务规则限制）、过滤条件（安审风控）、权限与角色结构。



#### 不当输出处理（LLM05 Improper Output Handing）

攻击者在未使用参数化查询的情况下执行LLM生成的SQL查询，导致SQL注入。

攻击者使用LLM输出构造文件路径，未进行适当清理时可能导致路径遍历漏洞。

攻击者将LLM生成的内容用于电子邮件模板，未进行适当转义时可能导致钓鱼攻击。

攻击者利用LLM的输出直接输入系统命令执行，导致远程代码执行。

攻击者利用LLM生成JavaScript或Markdown代码并返回给用户，代码被浏览器解释后引发XSS攻击。



#### 模型数据投毒（LLM04 Model Data Poisoning）

模型的训练数据很多来自外部未经验证的数据源，这些数据里可能还有坏数据。

攻击者可能会在训练数据中刻意埋入毒性数据，导致模型训练结果不可控。这类风险一般发生在训练阶段。



#### 供应链风险（LLM03 Supply Chain）

这类风险包括第三方组件的传统漏洞风险，使用的基座模型安全性弱，设备端硬件漏洞等。

还包括开源模型和组件的许可风险，隐私条款等。




# 大模型漏洞

## 模型层安全

### 数据投毒

1. 开源第三方数据集投毒，企业没有进行良好的数据清洗

2. 大模型发布后重新收集用户的使用数据进行重新训练，也可以完成投毒操作

### 后门植入

1. 第三方开源的大模型被后门植入，可能导致大模型使用方的隐私泄露

比如后门程序将返回的内容进行关键拼接，将信息发送到攻击者的服务器上

### 对抗攻击

又称越狱

用户输入一段专为大模型构造的提示词，绕过LLM的安全或者道德模块，使其能够看到非法内容

利用GPT的本能规避和记忆上下文等进行注入



### 数据泄露

1. 训练的数据集未脱敏

2. LLM本身的关键信息未脱敏

## 框架层安全

二进制安全，一般大模型都是Python为主的各类接口来访问底层的C++实现，攻击者可能通过开发者追求效率而舍弃安全的情况进行内存注入。



处理不可信数据：原始训练数据和序列化存储模型

利用模型处理文件数据的过程进行攻击

反序列化模型中执行恶意代码





训练集群中的主机暴露接口

## 应用层安全

### 传统web安全

未授权、身份验证等问题，攻击者可以访问其他人的敏感数据



或者是通信数据直接明文存储在公网上





大模型本身的漏洞处理一些经过设计的文件会触发漏洞进而进行攻击





沙箱逃逸和任意代码执行







